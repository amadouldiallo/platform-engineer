# ğŸ—ï¸ Infrastructure GCP - Terraform

Ce module Terraform provisionne l'infrastructure GCP nÃ©cessaire pour la plateforme cloud-native.

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GCP Project: kkgcplabs01-009                          â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     Terraform State (GCS)                               â”‚ â”‚
â”‚  â”‚                gs://kkgcplabs01-009-tf-state                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      VPC Network: platform-vpc                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚              Subnet: platform-vpc-subnet (us-central1)            â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â€¢ Primary CIDR: 10.0.0.0/20                                     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â€¢ Pods CIDR: 10.16.0.0/14 (secondary range)                     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â€¢ Services CIDR: 10.20.0.0/20 (secondary range)                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                                   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚     GKE Cluster: platform-cluster (us-central1-a)           â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚              Node Pool: default-pool                  â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â€¢ Machine: e2-medium                                 â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â€¢ Disk: 20GB (org policy limit: 50GB)                â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â€¢ Nodes: 1 (zonal cluster)                           â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â€¢ Private nodes (no public IP)                       â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â€¢ Shielded nodes enabled                             â”‚  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                             â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  Features:                                                  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  âœ“ Workload Identity                                        â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  âœ“ Network Policy (Calico)                                  â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  âœ“ Managed Prometheus                                       â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  âœ“ VPC-native networking                                    â”‚  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚ â”‚
â”‚  â”‚  â”‚  Cloud Router   â”‚â”€â”€â”€â”€â”‚    Cloud NAT    â”‚â”€â”€â”€â”€ Internet               â”‚ â”‚
â”‚  â”‚  â”‚ platform-vpc-   â”‚    â”‚ platform-vpc-   â”‚                            â”‚ â”‚
â”‚  â”‚  â”‚    router       â”‚    â”‚      nat        â”‚                            â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  Firewall Rules:                                                             â”‚
â”‚  â€¢ allow-internal: TCP/UDP/ICMP between VPC resources                       â”‚
â”‚  â€¢ allow-health-checks: GCP Load Balancer health checks                     â”‚
â”‚  â€¢ allow-ssh: IAP tunnel for secure SSH access                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš ï¸ Adaptations pour Projet Lab GCP

Ce code a Ã©tÃ© adaptÃ© pour fonctionner avec les **contraintes d'organisation** des projets lab GCP :

| Contrainte Org Policy | ProblÃ¨me | Solution appliquÃ©e |
|-----------------------|----------|-------------------|
| `gcp.resourceLocations` | Europe interdite | â†’ RÃ©gion `us-central1` |
| `custom.allowEssentialVPCFlowLogs` | Flow logs > 30% interdit | â†’ `flow_sampling = 0.1` (10%) |
| `custom.denyCloudNATLogging` | NAT logging interdit | â†’ Logging dÃ©sactivÃ© |
| `iam.serviceAccounts.create` | CrÃ©ation SA interdite | â†’ Service Account commentÃ© |
| `custom.allowedMaxDiskSize` | Disk > 50GB interdit | â†’ `disk_size_gb = 20` |
| Quota SSD limitÃ© (250GB) | Cluster rÃ©gional = 300GB | â†’ Cluster **zonal** (1 node) |
| Maintenance policy | FenÃªtre hebdo insuffisante | â†’ `FREQ=DAILY` |

> ğŸ’¡ **Note** : En production, rÃ©activez ces features en dÃ©commentant le code appropriÃ©.

---

## ğŸ”„ Quick Reset (Nouveau projet toutes les 3h)

Les projets lab GCP expirent toutes les 3 heures. Voici comment rÃ©initialiser rapidement :

### âš ï¸ Ã‰tape 0 : RÃ©authentification (OBLIGATOIRE)

> **Important** : Chaque nouveau projet lab = nouveau compte Google. Vous DEVEZ vous rÃ©authentifier !

```bash
# 1. RÃ©voquer TOUTES les authentifications de l'ancien projet
gcloud auth revoke --all

# 2. Se connecter avec le NOUVEAU compte (celui affichÃ© dans la console lab)
gcloud auth login
# â†’ Une fenÃªtre navigateur s'ouvre, connectez-vous avec le nouveau compte

# 3. Configurer les credentials pour Terraform
gcloud auth application-default login

# 4. VÃ©rifier que vous Ãªtes sur le bon projet
gcloud config get-value project
gcloud config get-value account
```

### Option 1 : Script automatique (recommandÃ©) âš¡

```bash
cd infra/

# RÃ©cupÃ©rer le nouveau project_id
gcloud config get-value project
# Output: kkgcplabs01-XXX

# Lancer le script avec le nouveau project_id
./init.sh kkgcplabs01-XXX

# Le script fait automatiquement :
# âœ… VÃ©rifie l'authentification
# âœ… CrÃ©e le bucket GCS
# âœ… Met Ã  jour backend.tf
# âœ… CrÃ©e terraform.tfvars
# âœ… Initialise Terraform

# DÃ©ployer (~7 min)
terraform apply
```

### Option 2 : Manuel

```bash
cd infra/

# 1. RÃ©cupÃ©rer le nouveau project_id
PROJECT_ID=$(gcloud config get-value project)
echo "Project: $PROJECT_ID"

# 2. CrÃ©er le bucket
gcloud storage buckets create gs://${PROJECT_ID}-tf-state \
  --location=us-central1 \
  --uniform-bucket-level-access

# 3. Mettre Ã  jour backend.tf (remplacer le bucket)
sed -i "s/bucket = \".*\"/bucket = \"${PROJECT_ID}-tf-state\"/" backend.tf

# 4. CrÃ©er terraform.tfvars depuis le template
sed "s/PROJECT_ID/${PROJECT_ID}/g" terraform.tfvars.example > terraform.tfvars

# 5. RÃ©initialiser Terraform
rm -rf .terraform
terraform init

# 6. DÃ©ployer
terraform apply
```

### ğŸš¨ Erreur courante : "Permission denied"

Si vous voyez cette erreur :
```
kk_lab_user_XXXXX@kkgcplabsXX.com does not have storage.buckets.create access
```

**Cause** : Vous Ãªtes encore authentifiÃ© avec l'ancien compte.

**Solution** :
```bash
gcloud auth revoke --all
gcloud auth login
gcloud auth application-default login
```

### ğŸ“‹ Checklist nouveau projet

| Ã‰tape | Commande | VÃ©rifiÃ© |
|-------|----------|---------|
| RÃ©voquer ancien compte | `gcloud auth revoke --all` | â˜ |
| Login nouveau compte | `gcloud auth login` | â˜ |
| Application credentials | `gcloud auth application-default login` | â˜ |
| VÃ©rifier projet | `gcloud config get-value project` | â˜ |
| Lancer init.sh | `./init.sh <PROJECT_ID>` | â˜ |
| DÃ©ployer | `terraform apply` | â˜ |

### Fichiers modifiÃ©s automatiquement

| Fichier | Variable | Exemple |
|---------|----------|---------|
| `backend.tf` | `bucket` | `kkgcplabs01-XXX-tf-state` |
| `terraform.tfvars` | `project_id` | `kkgcplabs01-XXX` |

---

## ğŸš€ Quick Start

### PrÃ©requis

| Outil | Version | Installation |
|-------|---------|--------------|
| Google Cloud SDK | Latest | `curl https://sdk.cloud.google.com \| bash` |
| Terraform | >= 1.5.0 | `brew install terraform` ou [tfenv](https://github.com/tfutils/tfenv) |
| kubectl | Latest | `gcloud components install kubectl` |
| gke-gcloud-auth-plugin | Latest | `gcloud components install gke-gcloud-auth-plugin` |

### 1ï¸âƒ£ Authentification GCP

```bash
# Se connecter Ã  GCP
gcloud auth login
gcloud auth application-default login

# VÃ©rifier le projet actuel
gcloud config get-value project

# Configurer le projet (si nÃ©cessaire)
gcloud config set project kkgcplabs01-009
```

### 2ï¸âƒ£ CrÃ©er le bucket pour Terraform State

> âš ï¸ **Important** : Le bucket doit Ãªtre crÃ©Ã© AVANT `terraform init`

```bash
# CrÃ©er le bucket GCS pour le state Terraform
gcloud storage buckets create gs://kkgcplabs01-009-tf-state \
  --project=kkgcplabs01-009 \
  --location=us-central1 \
  --uniform-bucket-level-access

# Activer le versioning (recommandÃ© pour la rÃ©cupÃ©ration)
gcloud storage buckets update gs://kkgcplabs01-009-tf-state --versioning
```

### 3ï¸âƒ£ DÃ©ploiement

```bash
cd infra/

# Initialiser Terraform (tÃ©lÃ©charge les providers, configure le backend)
terraform init

# VÃ©rifier le plan d'exÃ©cution
terraform plan

# Appliquer l'infrastructure (~ 7-10 minutes pour GKE zonal)
terraform apply

# RÃ©pondre 'yes' pour confirmer
```

### 4ï¸âƒ£ Connexion au cluster GKE

```bash
# Installer le plugin d'authentification (si pas dÃ©jÃ  fait)
gcloud components install gke-gcloud-auth-plugin

# Ou via apt (Ubuntu/Debian)
sudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin

# Configurer la variable d'environnement
export USE_GKE_GCLOUD_AUTH_PLUGIN=True

# Configurer kubectl (ATTENTION: cluster ZONAL, pas rÃ©gional)
gcloud container clusters get-credentials platform-cluster \
  --zone us-central1-a \
  --project kkgcplabs01-009

# VÃ©rifier la connexion
kubectl get nodes
kubectl cluster-info
```

---

## ğŸ“ Structure du projet

```
infra/
â”œâ”€â”€ main.tf                    # Orchestration principale + activation APIs
â”œâ”€â”€ variables.tf               # DÃ©finition des variables d'entrÃ©e
â”œâ”€â”€ outputs.tf                 # Outputs exportÃ©s (endpoints, commandes)
â”œâ”€â”€ providers.tf               # Configuration providers GCP + Kubernetes
â”œâ”€â”€ backend.tf                 # Backend GCS pour le state
â”œâ”€â”€ versions.tf                # Contraintes de versions Terraform/providers
â”œâ”€â”€ terraform.tfvars           # Valeurs des variables (ignorÃ© par git)
â”œâ”€â”€ terraform.tfvars.example   # Exemple de configuration
â”œâ”€â”€ terraform.md               # ğŸ“š Guide Terraform pour dÃ©butants
â”œâ”€â”€ .terraform.lock.hcl        # Lock file des providers
â””â”€â”€ modules/
    â”œâ”€â”€ network/               # ğŸŒ Module rÃ©seau
    â”‚   â”œâ”€â”€ main.tf            #    VPC, Subnet, Cloud Router, NAT, Firewalls
    â”‚   â”œâ”€â”€ variables.tf       #    Variables du module
    â”‚   â””â”€â”€ outputs.tf         #    Outputs (network_name, subnet_name, etc.)
    â””â”€â”€ gke/                   # â˜¸ï¸ Module GKE
        â”œâ”€â”€ main.tf            #    Cluster avec node pool intÃ©grÃ©
        â”œâ”€â”€ variables.tf       #    Variables du module
        â””â”€â”€ outputs.tf         #    Outputs (cluster_endpoint, etc.)
```

---

## âš™ï¸ Configuration

### Variables principales

| Variable | Description | Valeur actuelle |
|----------|-------------|-----------------|
| `project_id` | ID du projet GCP | `kkgcplabs01-009` |
| `region` | RÃ©gion GCP | `us-central1` |
| `zone` | Zone GCP (cluster zonal) | `us-central1-a` |
| `environment` | Environnement | `dev` |
| `cluster_name` | Nom du cluster GKE | `platform-cluster` |
| `node_count` | Nombre de nÅ“uds | `1` |
| `machine_type` | Type de machine | `e2-medium` |
| `disk_size_gb` | Taille disque | `20 GB` |

### Configuration rÃ©seau

| ParamÃ¨tre | CIDR | Usage |
|-----------|------|-------|
| Subnet primaire | `10.0.0.0/20` | NÅ“uds GKE |
| Pods (secondary) | `10.16.0.0/14` | ~262k IPs pour pods |
| Services (secondary) | `10.20.0.0/20` | ~4k IPs pour services K8s |
| Master CIDR | `172.16.0.0/28` | Control plane privÃ© |

### Cluster Zonal vs RÃ©gional

| Aspect | Zonal (actuel) | RÃ©gional |
|--------|---------------|----------|
| Haute disponibilitÃ© | âŒ Single zone | âœ… Multi-zone |
| CoÃ»t | ğŸ’° Moins cher | ğŸ’°ğŸ’°ğŸ’° 3x nodes |
| Quota SSD | âœ… ~100GB | âŒ ~300GB (dÃ©passÃ©) |
| Use case | Dev/POC | Production |

---

## ğŸ” SÃ©curitÃ©

### Workload Identity

Le cluster utilise **Workload Identity** pour une authentification sÃ©curisÃ©e aux services GCP :

```yaml
# Exemple: Lier un ServiceAccount K8s Ã  un ServiceAccount GCP
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app-sa
  namespace: default
  annotations:
    iam.gke.io/gcp-service-account: MY_SA@kkgcplabs01-009.iam.gserviceaccount.com
```

> âš ï¸ **Note Lab** : La crÃ©ation de Service Accounts GCP est dÃ©sactivÃ©e dans ce projet lab. Utilisez le default compute SA ou crÃ©ez manuellement.

### Private Cluster

| Feature | Configuration |
|---------|---------------|
| NÅ“uds privÃ©s | âœ… Pas d'IP publique |
| Endpoint public | âœ… API accessible (avec authorized networks) |
| Cloud NAT | âœ… AccÃ¨s internet sortant (logging dÃ©sactivÃ©) |
| IAP Tunnel | âœ… SSH sÃ©curisÃ© vers les nÅ“uds |

### Network Policies

Calico est activÃ© pour les NetworkPolicies Kubernetes :

```yaml
# Exemple: Isoler un namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

---

## ğŸ“Š Outputs Terraform

| Output | Description | Valeur |
|--------|-------------|--------|
| `cluster_name` | Nom du cluster | `platform-cluster` |
| `cluster_location` | Zone du cluster | `us-central1-a` |
| `cluster_endpoint` | Endpoint API (sensible) | `https://x.x.x.x` |
| `gke_connect_command` | Commande kubectl | `gcloud container clusters get-credentials...` |
| `workload_identity_pool` | Pool WI | `kkgcplabs01-009.svc.id.goog` |
| `network_name` | Nom du VPC | `platform-vpc` |
| `subnet_name` | Nom du subnet | `platform-vpc-subnet` |

```bash
# Afficher tous les outputs
terraform output

# Afficher un output spÃ©cifique
terraform output gke_connect_command
```

---

## ğŸ’° Estimation des coÃ»ts

| Ressource | SpÃ©cification | CoÃ»t estimÃ©/mois |
|-----------|---------------|------------------|
| GKE Cluster | Management fee | ~$72 |
| Node (1x e2-medium) | Zonal, 1 node | ~$25 |
| Cloud NAT | Gateway + data | ~$10-30 |
| Persistent Disk | 20GB | ~$1 |
| **Total estimÃ©** | | **~$110-130/mois** |

> ğŸ’¡ Le cluster zonal rÃ©duit les coÃ»ts de ~40% par rapport au rÃ©gional.

---

## ğŸ”§ Commandes utiles

### Terraform

```bash
# Formater le code
terraform fmt -recursive

# Valider la syntaxe
terraform validate

# Voir l'Ã©tat actuel
terraform state list

# RafraÃ®chir l'Ã©tat
terraform refresh

# DÃ©truire le cluster seulement
terraform destroy -target=module.gke
```

### GKE / kubectl

```bash
# Infos cluster
kubectl cluster-info
kubectl get nodes -o wide

# VÃ©rifier les composants systÃ¨me
kubectl get pods -n kube-system

# Voir les events
kubectl get events --sort-by='.lastTimestamp'
```

### Debugging

```bash
# Logs Terraform dÃ©taillÃ©s
TF_LOG=DEBUG terraform plan

# VÃ©rifier l'Ã©tat du cluster GKE (ZONAL)
gcloud container clusters describe platform-cluster \
  --zone us-central1-a \
  --format="table(status,currentNodeCount,currentMasterVersion)"

# VÃ©rifier les opÃ©rations en cours
gcloud container operations list --filter="status!=DONE"
```

---

## ğŸš¨ Troubleshooting

### Erreur: "bucket does not exist"

```bash
gcloud storage buckets create gs://kkgcplabs01-009-tf-state \
  --project=kkgcplabs01-009 \
  --location=us-central1 \
  --uniform-bucket-level-access
```

### Erreur: "gke-gcloud-auth-plugin not found"

```bash
# Installer le plugin
gcloud components install gke-gcloud-auth-plugin

# Ou via apt
sudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin

# Configurer la variable
export USE_GKE_GCLOUD_AUTH_PLUGIN=True
echo 'export USE_GKE_GCLOUD_AUTH_PLUGIN=True' >> ~/.bashrc
```

### Erreur: "VPC Flow logs policy"

Le sampling rate doit Ãªtre < 30%. DÃ©jÃ  corrigÃ© : `flow_sampling = 0.1`

### Erreur: "Cloud NAT logging not allowed"

Le logging NAT est dÃ©sactivÃ© dans le code pour respecter la policy.

### Erreur: "Maximum disk size is 50GB"

Utiliser `disk_size_gb = 20` (ou max 50). DÃ©jÃ  configurÃ©.

### Erreur: "SSD quota exceeded"

Utiliser un cluster zonal (1 zone) au lieu de rÃ©gional (3 zones). DÃ©jÃ  configurÃ© avec `zone = "us-central1-a"`.

### Erreur: "Permission denied to create service account"

La crÃ©ation de SA est commentÃ©e. Utilisez le default compute SA.

---

## ğŸ§¹ Nettoyage

```bash
# âš ï¸ ATTENTION: DÃ©truit TOUTES les ressources !
terraform destroy

# Supprimer aussi le bucket state (optionnel)
gcloud storage rm -r gs://kkgcplabs01-009-tf-state
```

---

## ğŸ“š Ressources

- [Terraform GCP Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs)
- [GKE Best Practices](https://cloud.google.com/kubernetes-engine/docs/best-practices)
- [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity)
- [Private GKE Clusters](https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept)
- [VPC-native Clusters](https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips)

---

## â¡ï¸ Prochaines Ã©tapes

Une fois l'infrastructure dÃ©ployÃ©e :

1. **BRIQUE 2** â€” GitOps avec FluxCD
2. **BRIQUE 3** â€” Crossplane pour le provisioning cloud
3. **BRIQUE 4** â€” Microservice + CI/CD
4. **BRIQUE 5** â€” ObservabilitÃ© + Developer Experience
